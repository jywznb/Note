**第二阶段****:**

考虑到组内对Flink计算引擎在Hudi入湖的潜在需求，围绕用Flink CDC 将DNC数据导入Hudi，实现Flink CDC和Spark Streaming两种计算引擎性能测试对比



**第二阶段****:**

验收以通过代码 review提高编程规范，导入程序实际可用来进行度量



**效果：成功将****NDC****数据以****Copy** **On** **Write****和****Merge** **On** **Read****两种方式的入湖**



**困难：**

•**官网上没有相关****API****的使用**

•**AVRO_SCHEMA****格式定义导入数据格式、实现****Kafka****解析****NDC****数据的解析**

•**完成入湖功能中，存在各种异常和报错，导致数据入湖回滚**



**解决：**

•**通过阅读源码中的测试代码学习相关****API****的使用**

•**Debug****源码来定位异常，定位****Hudi****源码来对比可能存在的问题**



**收获：虽然由于后续****server****端的开发，暂缓测试相关的跟进，但在实战中完成入湖功能，提升了自己解决问题的能力，较为深入的了解了数据湖**





Arctic是基于 hive 的实时数仓解决方案，统一了离线/实时数仓的存储层，可以用于构建一个场景通用，存算分离，流批一体的实时数仓系统，满足在业务上对数据实时性要求和低成本的要求。





项目功能完整的流程：

1、打开入湖任务（数据写到 kafka）

2、开启表内实时同步（kafka->arctic）

3、开启 hive 原生读取和 compaction（arctic->hive）

4、compaction 完成后可以在 hive 中查询



Aritic Server实现功能：

•同步任务启动和停止以及定时刷新任务状态信息

•获取和更新同步任务配置信息

•运行信息（任务列表）

•数据同步流量、延迟等变更信息

•Hive表和MySQL表数据字段映射

•MySQL数据库中所有表和表内字段获取

•完善元数据的校验

• 入湖任务支持查看详情功能



同步功能模块上线

•前后端联调

•同质量保障组（QA）保证代码的健壮性

•配合云音乐环境在NDP的部署和上线

•功能完善和补全





**效果：实现数据同步的需求，并完成代码优化，达到云音乐****POC****的要求**



**困难：**

• **同步任务启停相关逻辑的编写与优化**

• **在完成基本功能后，对代码中存在****bug****的修复**



**解决：**

• **通过review校验代码不规范的编写**

• 自主测试前端联调qa**测试完善存在的问题**



**收获：参加0.2.1版本数据湖产品的开发，熟悉开发过程（例如git命令使用、相关逻辑编写、代码健壮性的保证、代码部署上线）**