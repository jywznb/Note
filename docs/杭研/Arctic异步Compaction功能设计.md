Arctic 异步 Compaction 功能设计

 

# Background

Arctic 是基于 hive 的实时数仓解决方案，统一了离线/实时数仓的存储层，且可以不依赖于第三方索引实现 upsert/delete 功能。

 

不过，数仓的实时化会衍生出增量数据的小文件问题（实时性越高文件越碎片化），而对 upsert/delete 功能的支持则又提出了同一视图下数据一致性的要求（视图内需要做相同主键记录的合并）。异步 Compaction 是 Arctic 文件治理功能的一部分，目标是提供解决这两个问题的能力。此文档描述了异步 Compaction 相关的概念及具体实现流程。

 

此外，Arctic 还提出了基于主键 Hash 的 Arctic Tree 结构，用于组织数据湖的 base 及 delta 文件，以便进行高效的 Ingestion 及 Compaction。本文介绍的所有实现都是基于 Arctic Tree 结构之上来完成。

 

因此，查看本文的前置条件是需要了解 Arctic 相关的设计和实现。关于 Arctic 整体的功能设计以及 Arctic Tree 结构的详细介绍，请参考：《proposal: merge and compaction for arctic》（[https://docs.qq.com/doc/DR3JNb3RYcW9xQkZW](https://docs.qq.com/doc/DR3JNb3RYcW9xQkZW）。)）。

# Goals

如上所述，异步 Compaction 首要解决的是实时数仓的两个基本问题：

 

●   小文件问题

●   保证视图内数据一致性

 

此外，从易用性及性能等方面考虑，我们还需要考虑以下问题：

 

●   与 hive 原有功能兼容

●   高效 Merge

●   高效查询

# Concepts

关于 Compaction 相关的概念，之前整理过一份文件（https://docs.google.com/document/d/1nUI2KvYPQpfxtruYaPQTp4Kn4OYz2EqxybEZJR40MJc/edit?usp=sharing），但经过首次评审后做了一些修改。这里我把涉及到的几个关键概念再摘录下：

 

1、Arctic Tree 根据主键进行 hash。

2、文件可以无序。

3、文件元数据信息中包含 pk range（min_pk，max_pk）。

4、主键相交关系有 hash 相交（结点相交关系）和 range 相交（文件相交关系）。

5、文件写入位置有“趋下性”。

# 主要流程

这里我们暂时略过业务数据摄取及其他的流程，只关注 Compaction 相关的部分。

 

Ingestion 模块将数据写入相应分区的 Arctic Tree 结点之后，Compaction 服务主要会有以下几个步骤：

 

1. 扫描系统库获得所有分区 delta 数据
2. 依次对有 delta 数据的分区执行     Compaction Plan，生成执行计划[[1\]](#_msocom_1) [[2\]](#_msocom_2) 
3. 并发执行具体的 Compaction 操作 ——     Copy-On-Write
4. 更新系统库信息，完成 Compaction Commit
5. 拷贝现有分区数据，生成新的分区快照，更新查询视图

 

这里重点讲解下 Copy-On-Write、Compaction Commit 以及视图更新流程。

# 具体实现

## Copy-On-Write

Ingestion 写入的 delta 数据分散在 Arctic Tree 的各个结点当中。通常这些结点会处于树的同一层级，但某些情况下，部分写入结点会进行层级的上浮或者下沉操作。

 

Arctic Tree 是根据数据的主键 hash 再取模进行结点划分的。当数据进行上浮时，相当于两个子结点的数据二合一至父结点；而数据下沉时，相当于父结点的数据根据下一层级的 hash 规则进行了一拆二处理。

 

根据取模运算的规律和树形结构的特征，我们可以断定，任何结点内数据与其祖先及子孙结点数据都可能存在着主键相交关系，任何结点与其兄弟、堂兄弟结点数据主键都不相交。

 

结合主键 range 相交关系查找效率及尽量提升 Merge 操作并行度考虑，我们可以采用以下步骤执行 copy-on-write 操作：

### 1. 寻找最小相交结点（sub tree）

根据上述主键 hash 相交的规则，祖先结点和子孙结点之间互相隐含着相交关系，因此我们通常无法立即将其拆分。而兄弟和堂兄弟结点肯定互不相交，因此，我们可以先找到没有共同祖先结点的兄弟/堂兄弟结点集合（注：这里的结点都是指包含文件的非空结点而言，因此没有共同祖先结点也就是指没有包含文件的非空祖先结点）。也就是寻找一颗颗互不相交的子树，因为一旦找到子树的根结点，那就意味着其子孙结点都会与其相交，在这个范围内就不会有额外的子树，只能去兄弟/堂兄弟结点再去寻找。寻找完互不相交的子树后，我们就可以以子树为范围，划分一个个互不相交区域，区域之间并行执行任何操作彼此不会互相影响。这是第一层剪枝。

 

具体查找方法：我们可以通过前序遍历 Arctic Tree。寻找第一个非空（包含文件）的结点，此结点即为我们要查找子树的根结点。遍历完整棵树就可以找到所有的子树根结点。如果 Arctic Tree 的 root 结点也包含文件，则整棵树会作为一个相交区域，相当于第一步剪枝效果为零，这是极特殊情况。

![img](file:////Users/jiayangwei/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.jpg)

 

根据此方法，我们可以得到一串子树列表。

### 2. 寻找最小相交文件组（file group）

相交结点内的文件，虽然存在相交的可能性，但并不一定就是真正相交的。而根据结点内文件本身记录的 (min_pk, max_pk)，也即 pk range，我们可以进行进一步相交的准确判断，即判断两个文件 range 范围是否相交。这是第二层剪枝。

 

在上一步，我们确定互不相交的最小子树结构后，缩小了相交文件之间查找的范围。寻找相交文件就只需要在各个子树结构内进行。

 

关于互相相交文件的具体查找方法，我们首先可以假设每个文件为图的一个顶点，如果两个文件的 range 范围有重合，我们则说这两个文件所在的顶点是互相连通的。因此，我们可以用求解图的最大连通域算法来求解相交的文件集。求图的各个连通域算法我们采用的是 union-find 算法。

### 3. 并行度优化

划分完最小相交文件组之后，理论上已经可以进行文件合并的处理。但此时从性能及效率上考虑还并不是最优的。我们可以小心的分析及拆分相交文件之间的关系，并保证下述两点要求，以进一步提高任务执行的并行度：

 

1、 多个并行任务不会同时写一个文件

2、 任何文件内所有数据都得到妥善处理

 

这一步实现的具体方法：由于文件写入位置的趋下性，在一个最小相交文件组当中，我们总是会将上层结点位置的文件，按 hash 规则拆分至下层结点所在位置的同一层结点当中。如果下层结点所对应的兄弟结点不存在，则我们还需要先补足相应的兄弟结点（将会有数据写入）。具体流程可参考下图：

![img](file:////Users/jiayangwei/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image004.jpg)

经过这一步仔细拆分后（这部分只是在 plan 当中进行执行规划，并不会实际将数据文件进行打散拆分）。所有相交的上层结点文件会被正确的分配到相应的目标结点中去。

### 4. 生成执行 Task

上述的所有流程对应的都是 Plan 的过程，当所有文件都被正确的分配到相应的目标结点后，我们就可以依此生成具体的执行任务。我们把一个任务抽象为一个 Task，并提交到分布式计算集群（Flink）执行。

 

根据写入结点趋下性，所有目标任务结点都在叶子结点，他们之间彼此互不相交。因此我们可以以 file group 内的一个叶子结点作为一个 Task。首先，获取到叶子结点中和此 file group 所有相关的 base 文件以及 delta 文件，作为 Task 的输入，Task 输出的位置就是结点本身。

 

输入的 base 及 delta 文件我们会以 List 变量形式保存在 Task 类中。

### 5. 执行 Merge 流程

在一个 Task/结点 中，根据输入的 base 以及 delta 文件，执行以下流程进行合并：

 

1、 计算 base 文件 pk range 范围，并以排序后的 pk range 范围划分 pk 区间。

 

通常，base 文件之间 pk range 不会重合，一旦有重合的情况发生，则意味着后一个 range（q） 的 min_pk 必然小于前一个 range（p） 的 max_pk，因此这两个 range 无法划分成单独的区间，将会合并为一个区间，并以 p 的 min_pk，q 的 max_pk 作为新 range 的范围。

 

2、 每个区间对应生成一个 hashmap，用于放置 delta 文件数据。

 

考虑到 delta 文件可能过大，造成内存内无法同时放置所有delta 数据情况，此时 hashmap 可能需要采用基于磁盘的 k-v 数据库来实现，比如 rocksdb；

 

3、 按 delta 文件生成时间顺序，依次读取 delta 文件。

 

首先按照主键 hash 规则，将不属于此结点的数据丢弃，对于从属于当前结点的数据，则再次按照 range 范围，划分到具体的 hashmap 中去。如果 hashmap 中已有相同主键的数据，后续数据可直接覆盖。按此流程直至所有 delta 文件读完。

 

4、 按照 range 顺序依次读取 base 文件。

 

在读取每个 range 的 base file 之前，首先判断前一个区间的 hashmap 是否存在内容，如果存在，则首先需要输出前一个 hashmap 的内容。这里一个特殊的情况是读取第一个base file 时，它之前区间的 hashmap 存在内容，这时候需要创建一个新的文件，用于写入此 hashmap 数据，写完再关闭这个文件。其他情况下只需要在之前写入文件的最后追加完 hashmap 内容即可。

 

当前一个区间 hashmap 写入完毕后，关闭文件。并建一个新文件用于写入当前 base 及 区间 hashmap 的内容。base 数据以流式方式逐行读入，并在 hashmap 中查找判断，如果 hashmap 中不存在相同主键数据，则 base 数据直接输出；如果 hashmap 中存在相同主键数据，则分 2 中情况处理：1). 数据变更类型是 update，直接用 hashmap 数据替换 base 数据输出；2). 数据变更类型是 delete，则直接将数据丢弃。处理完这两种情况后都将 hashmap 中对应数据删除。

 

当处理完所有 base 文件后，要判断最大 max_pk 后面的那个区间 hashmap 是否存在数据，如果也存在，需要创建新的文件，并输出其包含的数据。

 

5、当执行完上述流程后，Merge 操作完成。

 

Merge 流程示意图：

 

![img](file:////Users/jiayangwei/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image006.jpg)

## 更新系统库

更新系统库，意味着 Compaction 操作执行 Commit。有几个点需要注意：

 

1、 提交所有新生成的 base 文件至系统库，并将其 snapshot id 赋值为对应的 delta snapshot id；

2、 将所有已合并的 base file 和 delta file 插入至系统库 expired_file 表，代表标记删除；

3、 查找所有 Compaction 未涉及到的 base 文件，将其复制一份，更新其 snapshot id 为对应的 delta snapshot id 后重新插入，以组成最新完整的 snapshot。

## 视图更新

上述所有操作完成后，用户并不会立即看到结果的变化。不直接对用户呈现结果一是为了保证数据的一致性，以避免用户看到中间态或错误的数据，二是为了异常情况下服务自身能比较容易的进行回滚等操作。

 

当达到视图更新的条件后，我们通过以下流程进行用户查询视图的变更：

### 1. 分区准备

根据我们涉及到的变更分区数量，新建相同数量的新分区目录。

### 2. 数据拷贝

为每个分区拷贝一份相应的数据。1、新生成的 base 文件可直接移动至相应新分区目录下；2、此次 Compaction 未涉及的 base 文件需要从原有分区拷贝一份至对应新分区目录。

### 3. 更新视图

修改 hive-metastore 对应分区的路径地址，使其指向新的分区目录，完成视图的更新。

 

通过以上原则，我们可以保证始终给用户提供稳定的视图。

## 数据清理

关于旧分区目录的清理时机，可能需要由业务方确定。因为 Compaction 服务无法了解还有哪些业务仍在读取旧的分区数据。

 

这需要在存储成本和用户体验之间做个权衡。保留更多的旧分区数据，意味着用户可以有更多的快照数目可选择，但也意味着部分相同数据会被存储为更多份，增加了存储成本。

# 思考及后续改进

在当前的设计实现当中，依旧遗留了几个问题，需要后续进行思考，看能否进一步做出改进和优化：

## 1、文件持续增大问题

根据文件 range 相交合并的原则，处于相同 range 区间内的数据都会写入至同一个文件当中。随着数据的增多，文件的大小可能会越来越大。

 

要控制文件的大小，有几个可行的方式。一是限制 min_pk，max_pk range 的范围，比如假设我们限制 min_pk，max_pk 范围为 [0, 9]，那这个文件最多就只有 10 条数据，文件就不会无限制的增大。但这个方案不现实的点在于一是我们无法很好的划分 range 范围，二是即使划分了范围，每条数据的大小依旧不可控，进而造成文件总大小也依旧不符合预期。

 

第二种实现方式是对文件进行拆分，一旦文件达到了我们设定的某个文件大小阈值，我们就将其一拆为二。这个方案的实现就是代价稍大，我们首先需要对文件的 pk 进行排序，以寻找出中位数 pk 值，然后以 pk 中位数进行划分数据拆成 2 个文件。

## 2、用户某些查询条件下相交文件过多

由于 Arctic Tree 采用的是主键 hash 再取模的方式构建二叉树结构。因此，除了查询条件以主键 hash 取模这一种，其他情况下的查询条件（比如根据某个其他字段进行过滤）涉及的文件都会对应到不同的 Arctic Tree 结点当中。而且，树的层数越多，结点分的越细（Arctic 写文件的趋势），单个查询涉及的文件就越多。

 

这种情况下在一定程度上可能会造成查询性能的下降。

## 3、与 hive bucket 功能的兼容

hive 支持分区下 bucket 功能，其具体实现为根据某个字段，在分区下根据用户指定的 bucket 数量将字段值取模后分配到相应的 bucket 文件当中。

 

Arctic 也是根据主键进行了 hash 取模，如果用户指定的 bucket 字段不是主键，bucket 数量和 Arctic 不一致，则这两个功能存在着互相冲突。因此，这两个功能如何共存也是后续需要考虑的问题。

 

------



不同partition可以并行执行吧，我建议把partition作为filegroup的一个判断条件，不同partition并发跑就可以了？



是的，partition 之间可以并行。这里的“依次”可能产生了歧义。

 

其实下面的所有流程都是针对单个 partition 而言的，多个分区采用相同的方式并行执行即可。